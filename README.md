干净的数据、有效的特征非常重要。特征可以学习得到，比如神经网络的第一层识别出图像的边边角角，第二层组合成花纹，轮廓，第三层就可以识别出物体，第四层就可以完成分类任务，将一个hard的任务分成几个步骤一点点学习到。

AlexNet：首次证明了学习到的特征可以超越⼿⼯设计的特征

maxpool相对avepool来说有稀疏参数的作用

用Dropout来控制全连接层的模型复杂度

relu的大于1的时候梯度恒为1，计算比sigmoid快；小于0的时候梯度为0，有起到了正则化、稀疏化的作用

缺点：没有提出简单的规则来指导后来的研究者如何设计新的网络，结构比较死板，很难复杂化或简单化

 

VGG：通过重复使⽤简单的基础块来构建深度模型。

多个VGG block（重复的模块）+全连接。通过增减VGG block来使模型复杂或简单化。

在VGG block中，3*3，stride=2的池化层使得尺寸大小减半；3*3，pad=1的卷积层使得图像大小不变

没经过一个VGG block，长宽都会减半

 

Nin：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络，“全连接”指1*1卷积代替的全连接

VGG和Alexnet大同小异，主要由卷积层（充分抽取空间特征）和全连接层（输出分类结果）两个结构构成。

Nin Block：没有使用全连接层，而是用1*1的卷积来代替，这样就不需要做flatten操作（全部都用卷积更方便，不需要在卷积和全连接之间转化），参数量减少，有防止过拟合的作用，但训练时间会增加？

1*1卷积核的作用：

没有用全连接层得出分类结果而是用global avepooling，为了和分类数相同，只能在通道数做手脚达到类似效果

 

GoogleNet：通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息。

Inception块组成

1*1卷积核是为了减少通道数，降低模型复杂度？
